# 优达学城(Udacity) - 机器学习算法工程师纳米学位
# 3- 非监督学习

快速浏览
- [首页](https://github.com/zhsam/Machine_Learning_Interview_Notes-Chinese)
- [机器学习基础](https://github.com/zhsam/Machine_Learning_Interview_Notes-Chinese/blob/master/1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md)
- [监督学习](https://github.com/zhsam/Machine_Learning_Interview_Notes-Chinese/blob/master/2-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.md)
- [非监督学习(本文)](https://github.com/zhsam/Machine_Learning_Interview_Notes-Chinese/blob/master/3-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.md)
- [深度学习](https://github.com/zhsam/Machine_Learning_Interview_Notes-Chinese/blob/master/4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.md)
- [强化学习](https://github.com/zhsam/Machine_Learning_Interview_Notes-Chinese/blob/master/5-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.md)
- [毕业项目]()
- [看面试经验]()
- [课程项目](https://github.com/zhsam/Udacity-MachineLearningEngineer-nd)

## 课程目录
- 聚类
- 聚类迷你项目
- 层次聚类法与密度聚类
- 高斯混合模型与聚类验证
- 特征缩放
- PCA 主成分分析
- PAC迷你项目
- 随机投影与ICA
- 非监督学习自我评估
- [项目] 创建客户细分


### 4-1 聚类
非监督学习：从没有标签(Flag)的数据中，发现其架构
聚类 Clustering：根据数据的分布方式，进行群集
降维 Dimensionality Reduction：从二维空间，变成一条线

最常见的聚类算法：K-means

### 4-3 层次聚类法
**单连接聚类法 (Single Link Clustering)**

**全连接聚类法 (Complete Link Clustering)**

**平均连接聚类法 (Average Link Clustering)**

**凝聚聚类法 (Aggolomerative Clustering)**

**离差平方和法 (WARD's method)**
- sklearn层次聚类的默认算法
- 计算A，B的方法

**层次聚类在Python中的实践**
- 使用层次聚类 - sklearn
```
from sklearn import datasets, cluster

# Load dataset (鸢尾花数据集)
X =  dataset.load_iris().date[:10]

# Specify the parameter for the Clustering. 'ward', linkage
# is default. Can alsouse 'complete' or 'average'
clust = cluster.AggolomerativeClustering(n_cluster=3, linkage='ward')

labels = clust.fit_predict(X)

#labels now contains an array representing with cluster
# each point belongs to : [1 0 0 0 1 2 0 1 0 0]
```
- 画出层次树(dendrograms) - scipy

```
from scipy.cluster.hierarchy import dendrograms, ward, single
from sklearn import datasets
import matplotlib.pyplot as plt

# Load dataset
X =  dataset.load_iris().date[:10]

# Perform Clustering
linkage_matrix = ward(X)

# Plot Dendrogram
dendrogram(linkage_matrix)

plt.show()
```
**层次聚类(Hierarchical Clustering)的优缺点、示例**
> 优缺点

优点：
- 层次聚类的结果信息丰富 (Resulting Hierarchical representation can be very informative)
- 可以额外进行可视化 (Provides an additional ability to visualize)
- 如果数据集实际存在层次信息，会更加有意义 (Especially potent when the dataset contains real hierarchical relationaships(e.g.Evolutional Biology))

缺点：
- 对离群值很敏感 (Sensitive to outliers)
- 计算复杂度高 (Computationally Intensive O(N^2) )
> 示例

- 应用层次聚类探索不同种类真菌的分泌蛋白 ([ Using Hierarchical Clustering of Secreted Protein Families to Classify and Rank Candidate Effectors of Rust Fungi](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029847))
- 应用全连接聚类法 绘制人体微生物的图表 (Association between composition of the human gastrointestinal microbiome and development of fatty liver with choline deficiency)

**Quiz**
Q：更容易导致细长的形状，而不一定是聚合或环形？
A：单连接算法
Q：分层聚类中的哪种连接方法是通过合并聚类来实现，并且这些聚类导致合并后在聚类中方差得到最小化的增加呢？
A：离差平方和法

**DBSCAN(Density-Based Special Clustering Applications with Noise)**
> 算法原理：

- 任意点作为起始点
- 检验邻近的 \epsilon (Epsilon)是否有其他点，如果没有视为噪声点
- 如果周围有点，根据阈值(MinPts)判定是否为噪声点(计算时包含自己)
- 如果有个点周围点的数量达到阈值，即形成一个聚成，该点称为核心点(Core Point)
- 分析该聚类下其余的点是否为核心点，非核心的点称为边界点(Border Point)

> 超参数：

- eps (Epsilon)
- min_samples (Minimum points to consider a cluster)

> 公式推导：

> 优缺点：

- 优点
  - 不需要设定要分成几种类别 (Don't need to specity the number of clusters)
  - 能灵活的找到各种形状、大小的类别 (Flexibility in the shapes & sizes of clusters)
  - 可以应付噪声、离群值 (Able to deal with noise, outliers)
- 缺点
  - 边界点由哪个聚类先发掘而定 (Border points that are reachable from two clusters)
    |-> 每次返回的聚类可能不同
  - 不容易找到不同密度的聚类 (Facing difficulty finding clusters of varying densities)
    |-> 使用 HDBSCAN

> 计算复杂度：

> 示例

- 网络流量聚类 ([Traffic Classification Using Clustering Algorithms](https://pages.cpsc.ucalgary.ca/~mahanti/papers/clustering.pdf))
- 温度异常检验 ([Anomaly detection in temperature data using dbscan algorithm](https://ieeexplore.ieee.org/abstract/document/5946052/))

> Python实践：
```
from sklearn impot datasets, cluster

# Load dataset
X = datasets.load_iris().data

# Specify the parameters for the clustering. These are the defaults.
db = cluster.DBSCAN(eps=0.5, min_samples=5)
db.fit(X)

# 'db.labels_' now contains an array representing which cluster each point
# belongs to. Samples labeled '-1' are noise.
```

**DBSCAN Quiz**
Q:DBSCAN 算法要求我们传递哪些输入？
A:Epsilon-定义没一点近邻的距离
Q:如下哪些选项是选择使用 DBSCAN 的好理由？
A:基于点的密度聚类数据集、识别数据集中的噪音

### 4-4 高斯混合模型与聚类验证
**高斯混合模型(Gaussian Mixture Model Clustering)**
每个点都隶属于每个聚类但是有不同的隶属度(Membership)。

> 算法原理：

**EM算法(Expectation-Maximization)**
1. 初始化K个高斯分布(Initialize K Gaussian Distributions)
2. [E步骤] 将数据软聚类成我们初始化的两个高斯 (Soft-Cluster data - "Expectation")
3. [M步骤] 基于软聚类重新评估高斯 (Re-estimate the Gaussians - "Maximization")
4. 评估对数似然来检查收敛(Evaluate Log-Likelihood to check for convergence)
5. (如果未收敛，重新回到步骤2)

步骤一: 初始化K个高斯分布
- 朴素方法：设为数据集本身的平均值
- 使用K-means的结果，初始化高斯分布
- 随机挑选Mean，Variance

步骤二： [E步骤]将数据软聚类成我们初始化的两个高斯
- 计算每个点在聚类A／聚类B的期望值

步骤三： [M步骤] 基于软聚类重新评估高斯
- 计算加权平均(weighted average)，获得新的高斯参数(均值、方差)

步骤四： 评估对数似然来检查收敛
- 对数似然(log-likelihood)越高，现有的模型越适用

> 超参数：

- 协方差类型 (convariance_type)
- 初始参数 (Initialization)

> 公式推导：


> 优缺点：

优点:
- 软聚类-多个聚类的示例性隶属度 (soft-clustering, sample membership of multiple clusters)
- 外观灵活度 (Cluster shape flexibility)

缺点：
- 对初始值敏感 (Sensitive to Initialization values)
- 可能收敛到局部最优 (Possible to converge to a local optimum)
- 收敛速度慢 (Slow convergence rate)

> 计算复杂度：

> 示例：

- [Nonparametric discovery of human routines from sensor data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.681.3152&rep=rep1&type=pdf)
- [Application of the Gaussian mixture model in pulsar astronomy](https://arxiv.org/abs/1205.6221)
- [Speaker Verification Using Adapted Gaussian Mixture Models](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.338&rep=rep1&type=pdf)
- [Adaptive background mixture models for real-time tracking](http://www.ai.mit.edu/projects/vsam/Publications/stauffer_cvpr98_track.pdf)
- [Video](https://www.youtube.com/watch?v=lLt9H6RFO6A)

> Python实践：

```
from sklearn import datasets, mixture

# Load dataset
X = datasets.load_iris().data[:10]

#Specify the parameters for the clustering
gmm = mixture.GaussianMixture(n_components=3)
gmm.fit(X)
clustering = gmm.predict(X)

# "clustering" now contains an array representing which
# point belogs to:
# [1 0 0 0 1 2 0 1 0 0 ]
```

**聚类分析流程(Cluster Analysis Process)**

1. 特征选取／提取(Feature Selection/Extraction)
  - 特征选取：从一组候选特征中选择特征
  - 特征提取: 对数据进行转换，生成新的有用特征，例：PCA
2. 选择一个聚类算法(Clustering Algorithm Selection&Tunning)
  - 邻近度度量(Proximity Measure)：欧式距离(Euclidean, 本课堂)、余弦距离(cosine distrance, 文档／词嵌入)、Pearson相关系数(Pearson's Correlation, 基因)
3. 聚类评价 Cluster Validation
  - 指数(indices)，每一个指数：聚类有效性指标(Clustering Validation index)
4. 聚类结果解释(Results Interpretation)

**聚类评估验证(Cluster Validation)**
- 外部指标 (External Indices):
  - 如果有标签
- 内部指标 (Internal Indices):
  - 仅使用数据来衡量数据和结构之间的吻合度(没有标签)
- 相对指标 (Relative Indices):
  - 表明两个聚类结构中，哪一个在某种意义上更好

大部分的指标是通过紧凑性(Compactness)、可分性(Separability)来定义的。
- 紧凑度: 一个聚类中，元素彼此之间的距离
- 可分性：不同聚类之间的距离

聚类的完美成果假设：
聚类中的元素彼此最相似、聚类之间最不相同

**外部评价指标**
- 兰德系数 (Adjusted Rand Score, [-1,1] (in sklean))
- Fawlks and Mallows [0,1] (in sklean)
- NMI measure [0,1] (in sklean)
- Jaccard [0,1] (in sklean)
- F-measure [0,1] (in sklean)
- Putiry [0,1]

**内部评价指标**
- 轮廓系数 (Silhouette index [-1, 1] (in sklean))
  - 公式：Silhouetee = b_i - a_i / max(a_i, b_i)
  - 缺点：
    - DBSCAN得分低：轮廓系数没有噪音的概念
    - 不奖励完美的切分两个环状结构

- Calinski-Harabasz (in sklean)
- BIC
- Dunn Index

DBSCAN的评估指标：
- [基于密度的聚类验证 - DBCV](https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/MLND+documents/10.1.1.707.9034.pdf)

### 4-5 特征缩放

**示例：T-shirt尺码**
Chirs - 140榜、6.1寸
Cameron - 175榜、5.9寸 - L号
Sarah - 115榜、5.2寸 - S号
请问Chris 应该穿什么号的衣服？

假设计算机的度量：身高+体重
Chirs = 146.1
Cameron = 180.9
Sarah = 120.2

这样的情况下，Chris的参数较靠近Sarah
问题：身高、体重的值大小非常不平衡
|-> 进行 特征缩放，把值缩放到0~1

**特征缩放的公式**
```
X' = X - Xmin / Xmax - Xmin
X' : 缩放后的值
```
例： [115, 140, 175]
X'140 = (140 - 115) / (175 - 115)

潜在问题：离群值会影像特征缩放的结果

**在Python中实践特征缩放**
- sklearn.preprocessing.MinMaxScale
```
from sklearn.preprocessing import MinMaxScaler
import numpy as np
weights = np.array([115.],[140.],[175.])
scaler = MinMaxScaler()
rescaled_weight = scaler.fit_transform(weights)
```
**特征缩放Quiz**

Q: 哪些机器学习算法会受到特征缩放的影响？ <br>
□ 决策树 <br>
v 使用 RBF 核函数的 SVM <br>
□ 线性回归 <br>
v K-均值聚类 <br>

解析：<br>
决策树：水平线，切分的区域相同 <br>
线性回归：相关系数会变，但是结果不变 <br>

### 4-6 PCA 主成分分析

**PCA, Principle Component Analysis**
- 将X轴移到变化的主轴(Principal Axis of Variation)
- 将y轴移到正交、重要性较低的变化方向(Orthogonal less important direction of variation)

- 信息损失：点到线(最大方差的线)的距离

**PCA的使用场景:**
- 找到数据的隐含特征
- 降维(Dimentionality Reduction)
  - 对高维度数据进行可视化
  - 降噪
  - 其他算法的预处理

**在Python中实践特征缩放**
```
def doPCA():
  from sklearn.decomposition import doPCA
  pca = PCA(n_components=2)
  pca.fit(data)
  return pca

pca = doPCA()
print(pca.explained_variance_ratio_)
first_pc = pca.components_[0]
second_pc = pca.components_[1]

transformed_data = pca.transform(data)
fot ii, ii in zip(transformed_data, data):
  plt.scatter( first_pc[0] * ii[0], first_pc[1]**i[0], color="r")
  plt.scatter( second_pc[0] * ii[0], second_pc[1]**i[0], color="c")
  plt.scatter( jj[0], jj[1], color="b")

plt.xlabel("bonus")
plt.ylabel("long-term incentive")
plt.show()
```



### 4-8 随机投影与ICA

### 补充- 时间序列分析
