# 优达学城(Udacity) - 机器学习算法工程师纳米学位
# 3- 非监督学习

快速浏览
- [首页](https://github.com/zhsam/Machine_Learning_Interview_Notes-Chinese)
- [机器学习基础](https://github.com/zhsam/Machine_Learning_Interview_Notes-Chinese/blob/master/1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md)
- [监督学习](https://github.com/zhsam/Machine_Learning_Interview_Notes-Chinese/blob/master/2-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.md)
- [非监督学习(本文)](https://github.com/zhsam/Machine_Learning_Interview_Notes-Chinese/blob/master/3-%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.md)
- [深度学习](https://github.com/zhsam/Machine_Learning_Interview_Notes-Chinese/blob/master/4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.md)
- [强化学习](https://github.com/zhsam/Machine_Learning_Interview_Notes-Chinese/blob/master/5-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.md)
- [毕业项目]()
- [看面试经验]()
- [课程项目](https://github.com/zhsam/Udacity-MachineLearningEngineer-nd)

## 课程目录
- 聚类
- 聚类迷你项目
- 层次聚类法与密度聚类
- 高斯混合模型与聚类验证
- 特征缩放
- PCA 主成分分析
- PAC迷你项目
- 随机投影与ICA
- 非监督学习自我评估
- [项目] 创建客户细分


### 4-1 聚类
非监督学习：从没有标签(Flag)的数据中，发现其架构
聚类 Clustering：根据数据的分布方式，进行群集
降维 Dimensionality Reduction：从二维空间，变成一条线

最常见的聚类算法：K-means

### 4-3 层次聚类法
**单连接聚类法 (Single Link Clustering)**

**全连接聚类法 (Complete Link Clustering)**

**平均连接聚类法 (Average Link Clustering)**

**凝聚聚类法 (Aggolomerative Clustering)**

**离差平方和法 (WARD's method)**
- sklearn层次聚类的默认算法
- 计算A，B的方法

**层次聚类在Python中的实践**
- 使用层次聚类 - sklearn
```
from sklearn import datasets, cluster

# Load dataset (鸢尾花数据集)
X =  dataset.load_iris().date[:10]

# Specify the parameter for the Clustering. 'ward', linkage
# is default. Can alsouse 'complete' or 'average'
clust = cluster.AggolomerativeClustering(n_cluster=3, linkage='ward')

labels = clust.fit_predict(X)

#labels now contains an array representing with cluster
# each point belongs to : [1 0 0 0 1 2 0 1 0 0]
```
- 画出层次树(dendrograms) - scipy

```
from scipy.cluster.hierarchy import dendrograms, ward, single
from sklearn import datasets
import matplotlib.pyplot as plt

# Load dataset
X =  dataset.load_iris().date[:10]

# Perform Clustering
linkage_matrix = ward(X)

# Plot Dendrogram
dendrogram(linkage_matrix)

plt.show()
```
**层次聚类(Hierarchical Clustering)的优缺点、示例**
> 优缺点

优点：
- 层次聚类的结果信息丰富 (Resulting Hierarchical representation can be very informative)
- 可以额外进行可视化 (Provides an additional ability to visualize)
- 如果数据集实际存在层次信息，会更加有意义 (Especially potent when the dataset contains real hierarchical relationaships(e.g.Evolutional Biology))

缺点：
- 对离群值很敏感 (Sensitive to outliers)
- 计算复杂度高 (Computationally Intensive O(N^2) )
> 示例

- 应用层次聚类探索不同种类真菌的分泌蛋白 ([ Using Hierarchical Clustering of Secreted Protein Families to Classify and Rank Candidate Effectors of Rust Fungi](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0029847))
- 应用全连接聚类法 绘制人体微生物的图表 (Association between composition of the human gastrointestinal microbiome and development of fatty liver with choline deficiency)

**Quiz**
Q：更容易导致细长的形状，而不一定是聚合或环形？
A：单连接算法
Q：分层聚类中的哪种连接方法是通过合并聚类来实现，并且这些聚类导致合并后在聚类中方差得到最小化的增加呢？
A：离差平方和法

**DBSCAN(Density-Based Special Clustering Applications with Noise)**
> 算法原理：

- 任意点作为起始点
- 检验邻近的 \epsilon (Epsilon)是否有其他点，如果没有视为噪声点
- 如果周围有点，根据阈值(MinPts)判定是否为噪声点(计算时包含自己)
- 如果有个点周围点的数量达到阈值，即形成一个聚成，该点称为核心点(Core Point)
- 分析该聚类下其余的点是否为核心点，非核心的点称为边界点(Border Point)

> 超参数：

- eps (Epsilon)
- min_samples (Minimum points to consider a cluster)

> 公式推导：

> 优缺点：

- 优点
  - 不需要设定要分成几种类别 (Don't need to specity the number of clusters)
  - 能灵活的找到各种形状、大小的类别 (Flexibility in the shapes & sizes of clusters)
  - 可以应付噪声、离群值 (Able to deal with noise, outliers)
- 缺点
  - 边界点由哪个聚类先发掘而定 (Border points that are reachable from two clusters)
    |-> 每次返回的聚类可能不同
  - 不容易找到不同密度的聚类 (Facing difficulty finding clusters of varying densities)
    |-> 使用 HDBSCAN

> 计算复杂度：

> 示例

- 网络流量聚类 ([Traffic Classification Using Clustering Algorithms](https://pages.cpsc.ucalgary.ca/~mahanti/papers/clustering.pdf))
- 温度异常检验 ([Anomaly detection in temperature data using dbscan algorithm](https://ieeexplore.ieee.org/abstract/document/5946052/))

> Python实践：
```
from sklearn impot datasets, cluster

# Load dataset
X = datasets.load_iris().data

# Specify the parameters for the clustering. These are the defaults.
db = cluster.DBSCAN(eps=0.5, min_samples=5)
db.fit(X)

# 'db.labels_' now contains an array representing which cluster each point
# belongs to. Samples labeled '-1' are noise.
```

**DBSCAN Quiz**
Q:DBSCAN 算法要求我们传递哪些输入？
A:Epsilon-定义没一点近邻的距离
Q:如下哪些选项是选择使用 DBSCAN 的好理由？
A:基于点的密度聚类数据集、识别数据集中的噪音

### 4-4 高斯混合模型与聚类验证

### 4-5 特征缩放

**示例：T-shirt尺码**
Chirs - 140榜、6.1寸
Cameron - 175榜、5.9寸 - L号
Sarah - 115榜、5.2寸 - S号
请问Chris 应该穿什么号的衣服？

假设计算机的度量：身高+体重
Chirs = 146.1
Cameron = 180.9
Sarah = 120.2

这样的情况下，Chris的参数较靠近Sarah
问题：身高、体重的值大小非常不平衡
|-> 进行 特征缩放，把值缩放到0~1

**特征缩放的公式**
```
X' = X - Xmin / Xmax - Xmin
X' : 缩放后的值
```
例： [115, 140, 175]
X'140 = (140 - 115) / (175 - 115)

潜在问题：离群值会影像特征缩放的结果

**在Python中实践特征缩放**
- sklearn.preprocessing.MinMaxScale
```
from sklearn.preprocessing import MinMaxScaler
import numpy as np
weights = np.array([115.],[140.],[175.])
scaler = MinMaxScaler()
rescaled_weight = scaler.fit_transform(weights)
```
**特征缩放Quiz**

Q: 哪些机器学习算法会受到特征缩放的影响？ <br>
□ 决策树 <br>
v 使用 RBF 核函数的 SVM <br>
□ 线性回归 <br>
v K-均值聚类 <br>

解析：<br>
决策树：水平线，切分的区域相同 <br>
线性回归：相关系数会变，但是结果不变 <br>

### 4-6 PCA 主成分分析

### 4-8 随机投影与ICA
